{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3147c5-52c5-497a-be8d-649b53997d9f",
   "metadata": {},
   "source": [
    "# My journey exploring AI LLM - Part 1 - LangChain\n",
    "\n",
    "My first step is learning LangChain. I chose it because I consider it a middle-ground solution. Before diving in, I already know there are various LLM models like GPT-3.5-turbo, GPT-4, Llama, Gemini, Claude, and Mistral. LangChain provides a standardized and cohesive interface for integrating these diverse models.\n",
    "\n",
    "When I was merely just an observer of the buzz in the AI world particularly in chatbot, I had many questions\n",
    "\n",
    "- Can I use my own internal data to generate answers? I know it's possible, but how?\n",
    "- How can ChatGPT produce answers based on previous conversations? There must be a memoization mechanism, but how is it implemented?\n",
    "- ...\n",
    "\n",
    "I had many other \"Can I...?\" and \"How can...?\" questions. LangChain seems to have the tools to answer them such as\n",
    "\n",
    "- **Memory** - https://langchain-ai.github.io/langgraph/concepts/memory/ - This tool answers how ChatGPT maintains context in conversations\n",
    "- **Retrieval augmented generation (RAG)** - https://python.langchain.com/docs/concepts/rag/ - This tool answers my question about using my own data for generating responses.\n",
    "\n",
    "LangChain's diverse tooling allows me to focus on realizing my ideas instead of getting bogged down in implementation details. That’s why I called / considered it a middle-ground solution.\n",
    "\n",
    "## Super simple and basic LLM application\n",
    "\n",
    "In this section I just want to  “saying Hello” to LangChain and prove some of my thoughts about LangChain. The application is implemented using the Python programming language and I’ve installed all of the related Python packages and dependencies needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b30c0-0e12-4010-be20-6f830a37b4ef",
   "metadata": {},
   "source": [
    "### Interact with several LLM models\n",
    "\n",
    "Here I will prove my thoughts about LangChain provides a standardized and cohesive interface for integrating diverse models. And not only that, I will also learn some other basic functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08588b-e6d0-4c13-a763-e749cf67c9c1",
   "metadata": {},
   "source": [
    "#### OpenAI's LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2573cc9-38e5-4445-989a-a52ec5f7211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "response = llm.invoke(\"Explain to me shortly like I am 5 years old what is trigonometry in math?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c6cbd-449e-436e-9754-8578b900a847",
   "metadata": {},
   "source": [
    "#### Google's Gemini LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b2a19-e1bf-457a-b519-082282e3f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7)\n",
    "\n",
    "response = llm.invoke(\"Explain to me shortly like I am 5 years old what is trigonometry in math?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e201b602-a49b-4b1b-aacf-8431a0275ca3",
   "metadata": {},
   "source": [
    "In the two simple application examples above, there are three patterns:\n",
    "\n",
    "- **LLM object instantiation pattern**:\n",
    "  - Regardless of the LLM choice, there are always two primary parameters passed to the constructor of the chosen LLM class: `model` and `temperature`.\n",
    "- **LLM communication execution pattern**:\n",
    "  - Regardless of the LLM choice, once the object is created, the `invoke()` method will be available for communicating with the LLM. Of course, there are many other methods available.\n",
    "- **Response data structure pattern**:\n",
    "  - The response contains data with the same structure. There are data attributes like `content`, `additional_kwarg`, `response_metadata`, and others.\n",
    "\n",
    "These patterns demonstrate that LangChain provides a standardized and cohesive interface, allowing us to easily integrate any LLM we want in the same way and obtain a consistent response data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060fd7c-9f35-4d28-927b-522dbcc4f8b1",
   "metadata": {},
   "source": [
    "## Simple and very basic chatbot\n",
    "\n",
    "In this section, I'm learning to create a customer service helpdesk question-and-answer application. I will instruct/guide the LLM model to act as a customer service representative to provide support for refrigerator products purchased by customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e576d7-22be-4085-ab43-bc42b25dacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "model_behavior_description = \"\"\"\n",
    "You are a customer service helpdesk representative for a refrigerator product. Your name is Lidya.\n",
    "Your task is to assist customers in handling complaints about refrigerators they have purchased.\n",
    "\n",
    "To make it easier for you to handle customer complaints,\n",
    "use the product information for the Samsung refrigerator model \"Bespoke Counter Depth 4-Door Flex™ Refrigerator (23 cu. ft.) with AI Family Hub™+ and AI Vision Inside™ in Stainless Steel\".\n",
    "\n",
    "Here are some key features of this refrigerator:\n",
    "* **Flexible Cooling:**  The FlexZone™ allows you to switch between fridge and freezer modes for different compartments.\n",
    "* **AI Family Hub™+:**  A smart screen on the door for entertainment, family communication, and smart home control.\n",
    "* **AI Vision Inside™:** Cameras inside the refrigerator help you see what's inside without opening the door.\n",
    "* **Counter Depth Design:**  A sleek design that aligns with your countertops.\n",
    "* **Large Capacity:**  23 cubic feet of space to store all your food.\n",
    "\n",
    "When responding to customers, be polite, helpful, and informative.\n",
    "Analyze the customer's description to identify the potential issue.\n",
    "Suggest possible solutions or troubleshooting steps.\n",
    "If you are unsure of the answer, ask clarifying questions to better understand the problem.\n",
    "\n",
    "Here are some examples of customer interactions:\n",
    "\n",
    "Customer: My refrigerator is making a strange noise.\n",
    "Lidya:  Can you describe the noise? Is it a buzzing, clicking, or something else? When does the noise occur?\n",
    "\n",
    "Customer: The ice maker isn't working.\n",
    "Lidya:  Have you checked if the ice maker is turned on? Is the water line connected properly?\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=model_behavior_description),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"xyz007\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619bf149-beed-4afd-9060-344650ad8355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! My name is Lidya, and I am here to assist you with any concerns you have regarding your Samsung Bespoke Counter Depth 4-Door Flex™ Refrigerator. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hello! I am Bob, Who are you?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89304a2c-09c3-4f01-b811-3fd7afba5bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lidya: I'm sorry to hear that your refrigerator is not cooling properly. This issue could be due to a few reasons. First, please check if the temperature settings are correctly set for the fridge and freezer compartments. Ensure that the vents inside the refrigerator are not blocked by food items, as this can restrict airflow.\n",
      "\n",
      "If the temperature settings are correct and the vents are clear, you can also check if the door is sealing properly. A faulty door seal can lead to warm air entering the refrigerator, affecting its cooling efficiency.\n",
      "\n",
      "If you've checked these and the issue persists, it might be a problem with the compressor or the cooling system. In that case, I recommend contacting our service center for further assistance. Would you like guidance on how to check any of these potential issues?\n"
     ]
    }
   ],
   "source": [
    "query = \"My refrigerator seems to be running fine, but it's not cooling. Why could this be?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d9907-7891-4816-ae33-e3b5f9559b2e",
   "metadata": {},
   "source": [
    "## Prompt Templates - Guiding a model's behavior\n",
    "\n",
    "I recall seeing several application-based digital products on the market that embed or integrate AI capabilities. Examples of such digital products include: customer service helpdesks, doctor consultations, lyric generation, and others. There are even products that train LLM models with tabular data, not just text data, which is typically used in ERP business applications, for example, in the Sales module to perform predictive analysis. This allows us to ask questions like, 'Why did sales decrease in March?'.\n",
    "\n",
    "### What is Prompt Templates?\n",
    "\n",
    "Prompt Templates is one of LangChain's feature that help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "LLM AI technology has created a new job role: **Prompt Engineering**. This means that there are still patterns and structures that must be applied (you can't just chat randomly). Prompt engineering is key to LLM application development because the quality of a prompt directly affects the LLM’s output. With the right prompts, you can ensure your app delivers accurate, relevant, and reliable responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
