{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3147c5-52c5-497a-be8d-649b53997d9f",
   "metadata": {},
   "source": [
    "# My journey exploring AI LLM - Part 1 - LangChain\n",
    "\n",
    "My first step is learning LangChain. I chose it because I consider it a middle-ground solution. Before diving in, I already know there are various LLM models like GPT-3.5-turbo, GPT-4, Llama, Gemini, Claude, and Mistral. LangChain provides a standardized and cohesive interface for integrating these diverse models.\n",
    "\n",
    "When I was merely just an observer of the buzz in the AI world particularly in chatbot, I had many questions\n",
    "\n",
    "- Can I use my own internal data to generate answers? I know it's possible, but how?\n",
    "- How can ChatGPT produce answers based on previous conversations? There must be a memoization mechanism, but how is it implemented?\n",
    "- ...\n",
    "\n",
    "I had many other \"Can I...?\" and \"How can...?\" questions. LangChain seems to have the tools to answer them such as\n",
    "\n",
    "- **Memory** - https://langchain-ai.github.io/langgraph/concepts/memory/ - This tool answers how ChatGPT maintains context in conversations\n",
    "- **Retrieval augmented generation (RAG)** - https://python.langchain.com/docs/concepts/rag/ - This tool answers my question about using my own data for generating responses.\n",
    "\n",
    "LangChain's diverse tooling allows me to focus on realizing my ideas instead of getting bogged down in implementation details. That’s why I called / considered it a middle-ground solution.\n",
    "\n",
    "## Super simple and basic and useless LLM application\n",
    "\n",
    "In this section I just want to  “saying Hello” to LangChain and prove some of my thoughts about LangChain. The application is implemented using the Python programming language and I’ve installed all of the related Python packages and dependencies needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b30c0-0e12-4010-be20-6f830a37b4ef",
   "metadata": {},
   "source": [
    "### Interact with several LLM models\n",
    "\n",
    "Here I will prove my thoughts about LangChain provides a standardized and cohesive interface for integrating diverse models. And not only that, I will also learn some other basic functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08588b-e6d0-4c13-a763-e749cf67c9c1",
   "metadata": {},
   "source": [
    "#### OpenAI's LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2573cc9-38e5-4445-989a-a52ec5f7211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "response = llm.invoke(\"Explain to me shortly like I am 5 years old what is trigonometry in math?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c6cbd-449e-436e-9754-8578b900a847",
   "metadata": {},
   "source": [
    "#### Google's Gemini LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b2a19-e1bf-457a-b519-082282e3f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7)\n",
    "\n",
    "response = llm.invoke(\"Explain to me shortly like I am 5 years old what is trigonometry in math?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e201b602-a49b-4b1b-aacf-8431a0275ca3",
   "metadata": {},
   "source": [
    "In the two simple application examples above, there are three patterns:\n",
    "\n",
    "- **LLM object instantiation pattern**:\n",
    "  - Regardless of the LLM choice, there are always two primary parameters passed to the constructor of the chosen LLM class: `model` and `temperature`.\n",
    "- **LLM communication execution pattern**:\n",
    "  - Regardless of the LLM choice, once the object is created, the `invoke()` method will be available for communicating with the LLM. Of course, there are many other methods available.\n",
    "- **Response data structure pattern**:\n",
    "  - The response contains data with the same structure. There are data attributes like `content`, `additional_kwarg`, `response_metadata`, and others.\n",
    "\n",
    "These patterns demonstrate that LangChain provides a standardized and cohesive interface, allowing us to easily integrate any LLM we want in the same way and obtain a consistent response data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060fd7c-9f35-4d28-927b-522dbcc4f8b1",
   "metadata": {},
   "source": [
    "## Simple and very basic chatbot\n",
    "\n",
    "In this section, I'm learning to create a customer service helpdesk question-and-answer application. I will instruct/guide the LLM model to act as a customer service representative to provide support for refrigerator products purchased by customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e576d7-22be-4085-ab43-bc42b25dacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "model_behavior_description = \"\"\"\n",
    "You are a customer service helpdesk representative for a refrigerator product. Your name is Lidya.\n",
    "Your task is to assist customers in handling complaints about refrigerators they have purchased.\n",
    "\n",
    "To make it easier for you to handle customer complaints,\n",
    "use the product information for the Samsung refrigerator model \"Bespoke Counter Depth 4-Door Flex™ Refrigerator (23 cu. ft.) with AI Family Hub™+ and AI Vision Inside™ in Stainless Steel\".\n",
    "\n",
    "Here are some key features of this refrigerator:\n",
    "* **Flexible Cooling:**  The FlexZone™ allows you to switch between fridge and freezer modes for different compartments.\n",
    "* **AI Family Hub™+:**  A smart screen on the door for entertainment, family communication, and smart home control.\n",
    "* **AI Vision Inside™:** Cameras inside the refrigerator help you see what's inside without opening the door.\n",
    "* **Counter Depth Design:**  A sleek design that aligns with your countertops.\n",
    "* **Large Capacity:**  23 cubic feet of space to store all your food.\n",
    "\n",
    "When responding to customers, be polite, helpful, and informative.\n",
    "Analyze the customer's description to identify the potential issue.\n",
    "Suggest possible solutions or troubleshooting steps.\n",
    "If you are unsure of the answer, ask clarifying questions to better understand the problem.\n",
    "\n",
    "Here are some examples of customer interactions:\n",
    "\n",
    "Customer: My refrigerator is making a strange noise.\n",
    "Lidya:  Can you describe the noise? Is it a buzzing, clicking, or something else? When does the noise occur?\n",
    "\n",
    "Customer: The ice maker isn't working.\n",
    "Lidya:  Have you checked if the ice maker is turned on? Is the water line connected properly?\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=model_behavior_description),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"xyz007\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619bf149-beed-4afd-9060-344650ad8355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! My name is Lidya, and I am here to assist you with any concerns you have regarding your Samsung Bespoke Counter Depth 4-Door Flex™ Refrigerator. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hello! I am Bob, Who are you?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89304a2c-09c3-4f01-b811-3fd7afba5bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lidya: I'm sorry to hear that your refrigerator is not cooling properly. This issue could be due to a few reasons. First, please check if the temperature settings are correctly set for the fridge and freezer compartments. Ensure that the vents inside the refrigerator are not blocked by food items, as this can restrict airflow.\n",
      "\n",
      "If the temperature settings are correct and the vents are clear, you can also check if the door is sealing properly. A faulty door seal can lead to warm air entering the refrigerator, affecting its cooling efficiency.\n",
      "\n",
      "If you've checked these and the issue persists, it might be a problem with the compressor or the cooling system. In that case, I recommend contacting our service center for further assistance. Would you like guidance on how to check any of these potential issues?\n"
     ]
    }
   ],
   "source": [
    "query = \"My refrigerator seems to be running fine, but it's not cooling. Why could this be?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0df23-897f-4f7b-ae44-972a468c30b7",
   "metadata": {},
   "source": [
    "Pada contoh aplikasi sangat sederhana di atas, hanya baris kode `llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)` yang saya kenal dan mengerti artinya. Sisanya banyak baris kode yang belum saya mengerti diantaranya: blok kode yang memuat `ChatPromptTemplate`, lalu `StateGraph` yang jika dilihat sekilas dari sumber importnya merupakan komponen dari package `langgraph`, `trim_messages()` dan `MemorySaver`.\n",
    "\n",
    "Saya akan pelajari terlebih dahulu semua hal-hal baru di atas. Jika sudah memahaminya saya akan mencoba untuk menulis pemahaman saya pada section-section selanjutnya di bawah."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac1342-ab45-4abf-a28b-3774d1326ec2",
   "metadata": {},
   "source": [
    "## Minimal common pattern for very basic application usefulness\n",
    "\n",
    "Setelah saya pelajari susunan kode dari aplikasi sangat sederhana yang sudah saya buat di section di atas sebelumnya, dan saya juga sudah mencoba beberapa tutorial dasar lainnya dari sumber-sumber lain. Saya melihat suatu **pola minimal** yang mana pola minimal ini harus dipenuhi untuk mewujudkan suatu aplikasi dapat dianggap berguna oleh penggunanya. Terutama pada perihal maintain context obrolan. Bayangkan jika aplikasinya tidak bisa menjawab sesuai konteks dari response pengguna.\n",
    "\n",
    "Menurut saya, dari pola minimal yang saya dapatkan, suatu aplikasi minimal harus memuat implementasi / penggunaan dari `ChatPromptTemplate` (bagian dari Prompt Templates), `StateGraph` (bagian dari LangGraph untuk state management), dan `MemorySaver` (bagian dari LangGraph untuk menyimpan state). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d9907-7891-4816-ae33-e3b5f9559b2e",
   "metadata": {},
   "source": [
    "### Prompt Templates - Guiding a model's behavior\n",
    "\n",
    "I recall seeing several application-based digital products on the market that embed or integrate AI capabilities. Examples of such digital products include: customer service helpdesks, doctor consultations, lyric generation, and others. There are even products that train LLM models with tabular data, not just text data, which is typically used in ERP business applications, for example, in the Sales module to perform predictive analysis. This allows us to ask questions like, 'Why did sales decrease in March?'.\n",
    "\n",
    "#### What is Prompt Templates?\n",
    "\n",
    "Prompt Templates is one of LangChain's feature that help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "LLM AI technology has created a new job role: **Prompt Engineering**. This means that there are still patterns and structures that must be applied (you can't just chat randomly). Prompt engineering is key to LLM application development because the quality of a prompt directly affects the LLM’s output. With the right prompts, you can ensure your app delivers accurate, relevant, and reliable responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febe136-7ca3-4240-a338-2b0635421867",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172561d6-67c7-43c7-bcd2-ddc3348f8e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm James, a barista here at the coffee shop. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import (SystemMessage, HumanMessage, AIMessage)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "model_behavior_description = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are a barista in a coffee shop. Your name is {barista_name}. \n",
    "Your job is to serve customers at the coffee shop where you work. \n",
    "You take orders for coffee drinks and answer questions about coffee.\n",
    "Tell your name if customer asking about your name or who are you.\n",
    "\n",
    "You have your own personal favorite coffee menu preference, \n",
    "and tell customer your personal favorite menu as recommendation to customer if customer don't know what to order.\n",
    "\n",
    "Your favorite coffee menu such as:\n",
    "{barista_recommended_coffee_menu}\n",
    "\n",
    "If there are any questions outside the topic of coffee, simply answer, \"I'm sorry, I don't understand your question.\" \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "barista_name = \"James\"\n",
    "barista_recommended_coffee_menu = \"\"\"\n",
    "1. Mochacino\n",
    "2. Tubruk espresso\n",
    "3. Vanilla latte\n",
    "4. Kopi Blandongan\n",
    "5. Kopi Kapal Api\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=model_behavior_description.format(\n",
    "        barista_name=barista_name, \n",
    "        barista_recommended_coffee_menu=barista_recommended_coffee_menu)\n",
    "    ),\n",
    "    HumanMessage(content=\"Hi! I am Joseph Leslie Armstrong. Who are you?\")\n",
    "])\n",
    "\n",
    "response = llm.invoke(prompt.messages)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d63dd4-b0f1-4355-a94d-76d8bed555fd",
   "metadata": {},
   "source": [
    "Copy blok kode `prompt_template` lalu copy juga response dari si model, lalu ubah / sesuaikan blok kodenya menjadi seperti berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f82449-b51d-4866-83b1-acfadebb3a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! One of my personal favorites is the Vanilla Latte. It's a delicious combination of espresso, steamed milk, and vanilla syrup. Would you like to try that?\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=model_behavior_description.format(\n",
    "        barista_name=barista_name, \n",
    "        barista_recommended_coffee_menu=barista_recommended_coffee_menu)\n",
    "    ),\n",
    "    HumanMessage(content=\"Hi! I am Joseph Leslie Armstrong. Who are you?\"),\n",
    "    AIMessage(content=\"Hello! I'm James, a barista here at the coffee shop. How can I assist you today?\"),\n",
    "    HumanMessage(content=\"I want to order but I am not sure what coffee I want. Can you recommend?\")\n",
    "])\n",
    "\n",
    "response = llm.invoke(prompt.messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6658d31-20ff-49cd-9e76-7a90e8313a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you're looking for a stronger coffee, you might enjoy our Tubruk Espresso. It's a bold and intense brew that packs a punch. Would you like to give that a try?\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=model_behavior_description.format(\n",
    "        barista_name=barista_name, \n",
    "        barista_recommended_coffee_menu=barista_recommended_coffee_menu)\n",
    "    ),\n",
    "    HumanMessage(content=\"Hi! I am Joseph Leslie Armstrong. Who are you?\"),\n",
    "    AIMessage(content=\"Hello! I'm James, a barista here at the coffee shop. How can I assist you today?\"),\n",
    "    HumanMessage(content=\"I want to order but I am not sure what coffee I want. Can you recommend?\"),\n",
    "    AIMessage(content=\"Of course! One of my personal favorites is the Vanilla Latte. It's a delicious combination of espresso, steamed milk, and vanilla syrup. Would you like to try that?\"),\n",
    "    HumanMessage(content=\"I think no. I think I want more strong coffee\")\n",
    "])\n",
    "\n",
    "response = llm.invoke(prompt.messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93270d-c9aa-43a1-9a60-6752705e0484",
   "metadata": {},
   "source": [
    "Tentu saja saya tidak mau menyusun obrolan dengan cara manual manual / mentah / \"bodoh\" seperti itu. Saya sengaja melakukan ini agar saya dapat benar-benar memahami bagaimana sebenarnya teknis mentahnya di belakang layar. Ya, seperti itu yang terjadi. Tinggal kita mencari cara bagaimana meng-otomatisasi-nya. Dari sini saya mengetahui tentu saja si `StateGraph` dan `MemorySaver` yang mempunyai peran untuk melakukan otomatisasinya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c8ed2-a738-49a8-a2a4-b5f4ebd8e108",
   "metadata": {},
   "source": [
    "### LangGraph - StateGraph, and Memory\n",
    "\n",
    "Setelah mengetahui \"hal bodoh\" dalam tanda kutip yang terjadi di belakang layar memperkuat pemahaman saya terhadap konsep tentang bagaimana model dapat memahami konteks sehingga dapat meresponse dengan baik dan tepat berdasarkan dengan response terakhir pengguna. Di section ini saya akan belajar bagaimana meng-otomatisasi untuk mempertahankan dan menambah historis obrolan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03fc59-cad7-4585-854b-3c01b22df09b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
